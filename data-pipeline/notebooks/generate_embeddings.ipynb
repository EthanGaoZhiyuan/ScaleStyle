{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from google.colab import files, drive\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "A-_kb8u1waEe"
      },
      "id": "A-_kb8u1waEe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "COMPETITION_NAME = \"h-and-m-personalized-fashion-recommendations\"\n",
        "# This is where we want the data to be available for our Spark code\n",
        "LOCAL_DATA_DIR = Path(\"/content/hm_data\")\n",
        "\n",
        "# This is where the persistent data lives in your Google Drive\n",
        "# NOTE: Ensure this path matches where you saved it last time!\n",
        "DRIVE_MOUNT_POINT = \"/content/drive\"\n",
        "DRIVE_PROJECT_PATH = Path(DRIVE_MOUNT_POINT) / \"MyDrive/ScaleStyle_Project/data\"\n",
        "# --- Configuration ---\n",
        "COMPETITION_NAME = \"h-and-m-personalized-fashion-recommendations\"\n",
        "# This is where we want the data to be available for our Spark code\n",
        "LOCAL_DATA_DIR = Path(\"/content/hm_data\")\n",
        "\n",
        "# This is where the persistent data lives in your Google Drive\n",
        "# NOTE: Ensure this path matches where you saved it last time!\n",
        "DRIVE_MOUNT_POINT = \"/content/drive\"\n",
        "DRIVE_PROJECT_PATH = Path(DRIVE_MOUNT_POINT) / \"MyDrive/ScaleStyle_Project/data\"\n",
        "ZIP_FILE_PATH = DRIVE_PROJECT_PATH / f\"{COMPETITION_NAME}.zip\""
      ],
      "metadata": {
        "id": "AzB0cnAwwl4b"
      },
      "id": "AzB0cnAwwl4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_data_from_drive():\n",
        "    \"\"\"\n",
        "    Mounts Google Drive and prepares the dataset.\n",
        "    1. Mounts Drive.\n",
        "    2. Checks if the ZIP file exists in Drive.\n",
        "    3. Extracts CSVs to the local Colab environment for fast access.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Mount Google Drive\n",
        "    if not os.path.exists(DRIVE_MOUNT_POINT):\n",
        "        print(\"ðŸ”¹ Mounting Google Drive...\")\n",
        "        drive.mount(DRIVE_MOUNT_POINT)\n",
        "    else:\n",
        "        print(\"âœ… Google Drive is already mounted.\")\n",
        "\n",
        "    # 2. Check for the dataset in Drive\n",
        "    if not ZIP_FILE_PATH.exists():\n",
        "        print(f\"âŒ Error: Dataset not found at {ZIP_FILE_PATH}\")\n",
        "        print(\"   Did you save it to a different folder last time?\")\n",
        "        print(\"   If this is a fresh start, you may need to run the download script once.\")\n",
        "        return\n",
        "\n",
        "    print(f\"âœ… Found cached dataset in Drive: {ZIP_FILE_PATH}\")\n",
        "\n",
        "    # 3. Extract to local environment (Faster IO than reading from Drive directly)\n",
        "    # We only extract if the target directory is empty or missing\n",
        "    if not LOCAL_DATA_DIR.exists():\n",
        "        print(\"ðŸ”¹ Extracting data to local Colab environment (this improves speed)...\")\n",
        "        LOCAL_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Unzip command is faster than Python zipfile\n",
        "        zip_source = str(ZIP_FILE_PATH)\n",
        "        target_dir = str(LOCAL_DATA_DIR)\n",
        "\n",
        "        # Extract specific CSVs to save space/time\n",
        "        !unzip -q -o \"{zip_source}\" \"articles.csv\" \"customers.csv\" \"transactions_train.csv\" -d \"{target_dir}\"\n",
        "\n",
        "        print(f\"âœ… Extraction complete! Data is ready at: {LOCAL_DATA_DIR}\")\n",
        "    else:\n",
        "        print(f\"âœ… Data already extracted at {LOCAL_DATA_DIR}. Skipping extraction.\")\n",
        "\n",
        "    print(f\"ðŸ“‚ Available files: {os.listdir(LOCAL_DATA_DIR)}\")\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    setup_data_from_drive()"
      ],
      "metadata": {
        "id": "5qfXgZolxbJ3"
      },
      "id": "5qfXgZolxbJ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# 1. Securely retrieve Token (will not be displayed on screen)\n",
        "try:\n",
        "    token = userdata.get(\"GITHUB_TOKEN\")\n",
        "    print(\"âœ… Token retrieved successfully\")\n",
        "except Exception:\n",
        "    print(\"âŒ Token not found. Please check the Secrets panel settings on the left\")\n",
        "\n",
        "# 2. Configure repository information\n",
        "username = \"EthanGaoZhiyuan\"\n",
        "repo = \"ScaleStyle\"\n",
        "\n",
        "# 3. Construct HTTPS URL with Token (Token used for authentication)\n",
        "# Format: https://token@github.com/username/repo.git\n",
        "clone_url = f\"https://{token}@github.com/{username}/{repo}.git\"\n",
        "\n",
        "# 4. Execute clone command\n",
        "# Use the -b flag to directly clone a specific branch\n",
        "branch = \"feat/phase2-embedding\"\n",
        "!git clone -b {branch} {clone_url}\n",
        "\n",
        "# 5. Verification\n",
        "if os.path.exists(repo):\n",
        "    print(f\"\\nðŸŽ‰ Code downloaded to: {repo}\")\n",
        "    !ls {repo}\n",
        "else:\n",
        "    print(\"\\nâŒ Clone failed. Please check Token permissions or repository existence\")"
      ],
      "metadata": {
        "id": "mR-06AW1xbrJ"
      },
      "id": "mR-06AW1xbrJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# 1. Change directory to the data-pipeline folder to access requirements.txt\n",
        "# This ensures pip installs the exact versions specified in your repo.\n",
        "repo_path = \"/content/ScaleStyle/data-pipeline\"\n",
        "os.chdir(repo_path)\n",
        "\n",
        "print(f\"ðŸ“‚ Current Working Directory: {os.getcwd()}\")\n",
        "\n",
        "# 2. Install dependencies\n",
        "# Using -q to keep the output clean.\n",
        "print(\"â¬‡ï¸ Installing dependencies from requirements.txt...\")\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# 3. Add the project root to system path\n",
        "# This allows Python to recognize 'src' as a module so we can do:\n",
        "# \"from src.feature_engineering import ...\"\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)\n",
        "    print(f\"âœ… Added {repo_path} to system path.\")\n",
        "\n",
        "print(\"âœ… Environment setup complete.\")"
      ],
      "metadata": {
        "id": "V-odJ1owx6Mx"
      },
      "id": "V-odJ1owx6Mx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATE EMBEDDINGS WITH GTE-QWEN2-7B (SOTA) ON A100 GPU\n",
        "\n",
        "MODEL SELECTION RATIONALE:\n",
        "I selected \"Alibaba-NLP/gte-Qwen2-7B-instruct\" for the following reasons:\n",
        "1. SOTA Performance: It currently holds top-tier positions on the MTEB(Massive Text Embedding Benchmark) leaderboard.\n",
        "2. LLM-Based Architecture: Unlike traditional BERT models (approx. 100M params), this model is based on Qwen2-7B (7 Billion params). It possesses deep semantic understanding of complex fashion descriptions rather than just keyword matching.\n",
        "3. Long Context Support: It supports up to 32k context window, allowing me to embed very long product details without truncation (unlike BERT's 512 limit).\n",
        "4. Hardware Utilization: I'm utilizing an NVIDIA A100 (80GB), which allows me to run this heavy model efficiently in FP16 precision."
      ],
      "metadata": {
        "id": "84_TKIas3B4g"
      },
      "id": "84_TKIas3B4g"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "CJwl69l36RpA"
      },
      "id": "CJwl69l36RpA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
        "BATCH_SIZE = 256\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "# Input/Output Paths (Google Drive)\n",
        "INPUT_PATH = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed/articles_parquet\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed/article_embeddings_bge.parquet\"\n",
        "\n",
        "# def last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "#     \"\"\"\n",
        "#     Extracts the embedding from the last token.\n",
        "#     For Causal LLMs (like Qwen) used as embedding models,\n",
        "#     pooling the last token (EOS) is the standard practice, unlike BERT which uses [CLS].\n",
        "#     \"\"\"\n",
        "#     left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
        "#     if left_padding:\n",
        "#         return last_hidden_states[:, -1]\n",
        "#     else:\n",
        "#         sequence_lengths = attention_mask.sum(dim=1) - 1\n",
        "#         batch_size = last_hidden_states.shape[0]\n",
        "#         # Gather the hidden state corresponding to the last real token\n",
        "#         return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
        "\n",
        "def cls_pooling(model_output):\n",
        "    \"\"\"\n",
        "    BGE/BERT Standard Pooling: Take the first token ([CLS])\n",
        "    \"\"\"\n",
        "    return model_output.last_hidden_state[:, 0]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Initializing SOTA model: {MODEL_NAME} on GPU...\")\n",
        "\n",
        "    # 1. Load Model & Tokenizer\n",
        "    # Note: 'trust_remote_code=True' is strictly required for Qwen2 custom architecture.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "    model = AutoModel.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        trust_remote_code=False,\n",
        "        torch_dtype=torch.float16,  # Use FP16 to save VRAM and accelerate on A100\n",
        "        device_map=\"cuda\"\n",
        "    )\n",
        "\n",
        "    # 2. Load Data\n",
        "    print(f\"Reading article data from: {INPUT_PATH}\")\n",
        "    df_articles = pd.read_parquet(INPUT_PATH)\n",
        "\n",
        "    # 3. Construct Input Text\n",
        "    # Since I'm embedding documents (products), I use the raw text.\n",
        "    # Instruction prompts (e.g., \"Given a query...\") are only needed during the retrieval phase.\n",
        "\n",
        "    # Preprocessing\n",
        "    df_articles['prod_name'] = df_articles['prod_name'].fillna(\"Unknown Product\")\n",
        "    df_articles['detail_desc'] = df_articles['detail_desc'].fillna(\"\")\n",
        "    df_articles['colour_group_name'] = df_articles['colour_group_name'].fillna(\"\")\n",
        "\n",
        "    # Concatenate fields to form a rich semantic representation\n",
        "    raw_texts = (\n",
        "        \"Product: \" + df_articles['prod_name'] + \". \" +\n",
        "        \"Description: \" + df_articles['detail_desc'] + \". \" +\n",
        "        \"Attributes: \" + df_articles['colour_group_name']\n",
        "    ).tolist()\n",
        "\n",
        "    print(f\"Total items to process: {len(raw_texts)}\")\n",
        "\n",
        "    # 4. Batch Inference Loop\n",
        "    all_embeddings = []\n",
        "\n",
        "    print(\"Start Batch Inference with Qwen2...\")\n",
        "    for i in tqdm(range(0, len(raw_texts), BATCH_SIZE)):\n",
        "        batch_texts = raw_texts[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Tokenize inputs\n",
        "        batch_dict = tokenizer(\n",
        "            batch_texts,\n",
        "            max_length=MAX_LENGTH,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_dict)\n",
        "\n",
        "            # Extracting\n",
        "            embeddings = cls_pooling(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
        "\n",
        "            # Normalizing\n",
        "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "            # Move to CPU to save GPU memory\n",
        "            all_embeddings.append(embeddings.float().cpu().numpy())\n",
        "\n",
        "    # 5. Save Results\n",
        "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "\n",
        "    # Save as a list column in the DataFrame\n",
        "    df_articles['embedding'] = list(final_embeddings)\n",
        "\n",
        "    # Keep only ID and Embedding columns for Milvus ingestion\n",
        "    output_df = df_articles[['article_id', 'embedding']]\n",
        "    output_df.to_parquet(OUTPUT_PATH)\n",
        "\n",
        "    print(f\"Embeddings saved to: {OUTPUT_PATH}\")\n",
        "    print(f\"CRITICAL NOTE: The vector dimension is {final_embeddings.shape[1]}.\")\n",
        "    print(f\"(Please ensure your Milvus Collection is created with dim={final_embeddings.shape[1]})\")"
      ],
      "metadata": {
        "id": "mkCKCjwNyWiq"
      },
      "id": "mkCKCjwNyWiq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3b4-WVG5blA"
      },
      "id": "k3b4-WVG5blA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "H100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}