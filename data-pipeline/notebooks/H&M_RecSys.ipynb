{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EthanGaoZhiyuan/ScaleStyle/blob/feat%2Fphase1-data-pipeline/data-pipeline/notebooks/H%26M_RecSys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmUtYYTVk4a8"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA DOWNLOAD & INITIAL ENVIRONMENT SETUP (COLAB RUNNER)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Purpose: Manages the secure download and extraction of the H&M dataset.\n",
    "# Assisted by: Gemini (AI) for Colab file system and CLI integration.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from google.colab import files, drive\n",
    "\n",
    "# --- Configuration ---\n",
    "# We define constants at the top for better maintainability (Senior Engineering practice)\n",
    "COMPETITION_NAME = \"h-and-m-personalized-fashion-recommendations\"\n",
    "DATA_DIR = Path(\"./hm_data\")\n",
    "DRIVE_MOUNT_PATH = Path(\"/content/drive\")\n",
    "DRIVE_PROJECT_PATH = DRIVE_MOUNT_PATH / \"MyDrive/ScaleStyle_Project/data\"\n",
    "\n",
    "\n",
    "def setup_kaggle_auth():\n",
    "    \"\"\"\n",
    "    Sets up Kaggle API authentication by asking the user to upload kaggle.json.\n",
    "    Moves the file to the correct location and sets permissions.\n",
    "    \"\"\"\n",
    "    kaggle_dir = Path.home() / \".kaggle\"\n",
    "    kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
    "\n",
    "    if kaggle_json_path.exists():\n",
    "        print(f\"‚úÖ Kaggle authentication found at {kaggle_json_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"üîπ Please upload your 'kaggle.json' file now...\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    if \"kaggle.json\" not in uploaded:\n",
    "        print(\"‚ùå Error: kaggle.json was not uploaded.\")\n",
    "        return\n",
    "\n",
    "    # Create .kaggle directory and move file\n",
    "    kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "    os.rename(\"kaggle.json\", kaggle_json_path)\n",
    "\n",
    "    # Security: Restrict file permissions (Linux command)\n",
    "    os.chmod(kaggle_json_path, 0o600)\n",
    "    print(\"‚úÖ Kaggle authentication setup complete.\")\n",
    "\n",
    "\n",
    "def download_and_extract(use_drive_cache: bool = True):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from Kaggle.\n",
    "\n",
    "    Args:\n",
    "        use_drive_cache (bool): If True, mounts Google Drive to persist the raw zip file\n",
    "                                so you don't have to download 30GB every time.\n",
    "    \"\"\"\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Option A: Use Google Drive to cache the huge zip file (Recommended)\n",
    "    if use_drive_cache:\n",
    "        print(\"üîπ Mounting Google Drive for persistent storage...\")\n",
    "        drive.mount(str(DRIVE_MOUNT_PATH))\n",
    "        DRIVE_PROJECT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        zip_path = DRIVE_PROJECT_PATH / f\"{COMPETITION_NAME}.zip\"\n",
    "\n",
    "        # Check if we already downloaded it to Drive\n",
    "        if not zip_path.exists():\n",
    "            print(f\"üîπ Downloading dataset to Google Drive ({zip_path})... this may take a while.\")\n",
    "            # Use Kaggle CLI to download to specific path\n",
    "            !kaggle competitions download -c {COMPETITION_NAME} -p {DRIVE_PROJECT_PATH}\n",
    "        else:\n",
    "            print(f\"‚úÖ Found cached dataset in Drive: {zip_path}\")\n",
    "\n",
    "    # Option B: Direct download to Colab (Ephemeral)\n",
    "    else:\n",
    "        print(\"üîπ Downloading dataset directly to Colab (Ephemeral storage)...\")\n",
    "        !kaggle competitions download -c {COMPETITION_NAME}\n",
    "        zip_path = Path(f\"{COMPETITION_NAME}.zip\")\n",
    "\n",
    "    # --- Extraction Phase ---\n",
    "    print(\"üîπ Extracting core CSV files (Skipping images for now to save space)...\")\n",
    "\n",
    "    # We use the system 'unzip' command which is faster than Python's zipfile for large files\n",
    "    # We explicitly ONLY extract the CSVs first. The images are too large (29GB+).\n",
    "    # We will handle image extraction later in the pipeline when needed.\n",
    "    zip_source = str(zip_path)\n",
    "    target_dir = str(DATA_DIR)\n",
    "\n",
    "    !unzip -q -o \"{zip_source}\" \"articles.csv\" \"customers.csv\" \"transactions_train.csv\" -d \"{target_dir}\"\n",
    "\n",
    "    print(f\"‚úÖ Extraction complete! Data is available at: {DATA_DIR}\")\n",
    "    print(f\"üìÇ Files: {os.listdir(DATA_DIR)}\")\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    setup_kaggle_auth()\n",
    "\n",
    "    # Set to True to save the zip to Drive (saves bandwidth on restarts)\n",
    "    # Set to False for a quick, one-off test\n",
    "    download_and_extract(use_drive_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Kyf-8zXrb1N",
    "outputId": "35ec309a-c1e2-4c21-8968-f935be18378f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google Drive is already mounted.\n",
      "‚úÖ Found cached dataset in Drive: /content/drive/MyDrive/ScaleStyle_Project/data/h-and-m-personalized-fashion-recommendations.zip\n",
      "‚úÖ Data already extracted at /content/hm_data. Skipping extraction.\n",
      "üìÇ Available files: ['articles.csv', 'customers.csv', 'transactions_train.csv']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA LOADER & SETUP (OPTIMIZED FOR GOOGLE DRIVE)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Purpose: Mounts Google Drive and loads the H&M dataset.\n",
    "#          Skips download if data already exists in Drive to save time.\n",
    "# ==============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "COMPETITION_NAME = \"h-and-m-personalized-fashion-recommendations\"\n",
    "# This is where we want the data to be available for our Spark code\n",
    "LOCAL_DATA_DIR = Path(\"/content/hm_data\")\n",
    "\n",
    "# This is where the persistent data lives in your Google Drive\n",
    "# NOTE: Ensure this path matches where you saved it last time!\n",
    "DRIVE_MOUNT_POINT = \"/content/drive\"\n",
    "DRIVE_PROJECT_PATH = Path(DRIVE_MOUNT_POINT) / \"MyDrive/ScaleStyle_Project/data\"\n",
    "ZIP_FILE_PATH = DRIVE_PROJECT_PATH / f\"{COMPETITION_NAME}.zip\"\n",
    "\n",
    "\n",
    "def setup_data_from_drive():\n",
    "    \"\"\"\n",
    "    Mounts Google Drive and prepares the dataset.\n",
    "    1. Mounts Drive.\n",
    "    2. Checks if the ZIP file exists in Drive.\n",
    "    3. Extracts CSVs to the local Colab environment for fast access.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Mount Google Drive\n",
    "    if not os.path.exists(DRIVE_MOUNT_POINT):\n",
    "        print(\"üîπ Mounting Google Drive...\")\n",
    "        drive.mount(DRIVE_MOUNT_POINT)\n",
    "    else:\n",
    "        print(\"‚úÖ Google Drive is already mounted.\")\n",
    "\n",
    "    # 2. Check for the dataset in Drive\n",
    "    if not ZIP_FILE_PATH.exists():\n",
    "        print(f\"‚ùå Error: Dataset not found at {ZIP_FILE_PATH}\")\n",
    "        print(\"   Did you save it to a different folder last time?\")\n",
    "        print(\"   If this is a fresh start, you may need to run the download script once.\")\n",
    "        return\n",
    "\n",
    "    print(f\"‚úÖ Found cached dataset in Drive: {ZIP_FILE_PATH}\")\n",
    "\n",
    "    # 3. Extract to local environment (Faster IO than reading from Drive directly)\n",
    "    # We only extract if the target directory is empty or missing\n",
    "    if not LOCAL_DATA_DIR.exists():\n",
    "        print(\"üîπ Extracting data to local Colab environment (this improves speed)...\")\n",
    "        LOCAL_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Unzip command is faster than Python zipfile\n",
    "        zip_source = str(ZIP_FILE_PATH)\n",
    "        target_dir = str(LOCAL_DATA_DIR)\n",
    "\n",
    "        # Extract specific CSVs to save space/time\n",
    "        !unzip -q -o \"{zip_source}\" \"articles.csv\" \"customers.csv\" \"transactions_train.csv\" -d \"{target_dir}\"\n",
    "\n",
    "        print(f\"‚úÖ Extraction complete! Data is ready at: {LOCAL_DATA_DIR}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Data already extracted at {LOCAL_DATA_DIR}. Skipping extraction.\")\n",
    "\n",
    "    print(f\"üìÇ Available files: {os.listdir(LOCAL_DATA_DIR)}\")\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    setup_data_from_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0lgMz5MqBkD",
    "outputId": "fd2edc08-d99c-40ad-d7ed-53909c252896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Token retrieved successfully\n",
      "fatal: destination path 'ScaleStyle' already exists and is not an empty directory.\n",
      "\n",
      "üéâ Code downloaded to: ScaleStyle\n",
      "data-pipeline\t    docs\t     inference-service\tREADME.md\n",
      "docker-compose.yml  gateway-service  infrastructure\n"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "# 1. Securely retrieve Token (will not be displayed on screen)\n",
    "try:\n",
    "    token = userdata.get(\"GITHUB_TOKEN\")\n",
    "    print(\"‚úÖ Token retrieved successfully\")\n",
    "except Exception:\n",
    "    print(\"‚ùå Token not found. Please check the Secrets panel settings on the left\")\n",
    "\n",
    "# 2. Configure repository information\n",
    "username = \"EthanGaoZhiyuan\"\n",
    "repo = \"ScaleStyle\"\n",
    "branch = \"feat/phase1-data-pipeline\"  # Specify the target branch\n",
    "\n",
    "# 3. Construct HTTPS URL with Token (Token used for authentication)\n",
    "# Format: https://token@github.com/username/repo.git\n",
    "clone_url = f\"https://{token}@github.com/{username}/{repo}.git\"\n",
    "\n",
    "# 4. Execute clone command\n",
    "# Use the -b flag to directly clone a specific branch\n",
    "!git clone -b {branch} {clone_url}\n",
    "\n",
    "# 5. Verification\n",
    "if os.path.exists(repo):\n",
    "    print(f\"\\nüéâ Code downloaded to: {repo}\")\n",
    "    !ls {repo}\n",
    "else:\n",
    "    print(\"\\n‚ùå Clone failed. Please check Token permissions or repository existence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7tJSAtUqyBZ",
    "outputId": "8f9caba9-d39a-4e9a-b2e8-368f4d9254b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current Working Directory: /content/ScaleStyle/data-pipeline\n",
      "‚¨áÔ∏è Installing dependencies from requirements.txt...\n",
      "‚úÖ Added /content/ScaleStyle/data-pipeline to system path.\n",
      "‚úÖ Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Change directory to the data-pipeline folder to access requirements.txt\n",
    "# This ensures pip installs the exact versions specified in your repo.\n",
    "repo_path = \"/content/ScaleStyle/data-pipeline\"\n",
    "os.chdir(repo_path)\n",
    "\n",
    "print(f\"üìÇ Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# 2. Install dependencies\n",
    "# Using -q to keep the output clean.\n",
    "print(\"‚¨áÔ∏è Installing dependencies from requirements.txt...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# 3. Add the project root to system path\n",
    "# This allows Python to recognize 'src' as a module so we can do:\n",
    "# \"from src.feature_engineering import ...\"\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "    print(f\"‚úÖ Added {repo_path} to system path.\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cgk8WzZArCsn",
    "outputId": "e4e2fc17-9ac6-473b-ce8a-90159a369629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session initialized.\n",
      "üìÇ Loading data from: /content/hm_data/customers.csv\n",
      "üìä Total Rows: 1371980\n",
      "üîπ Sample of raw data (Top 5):\n",
      "+--------------------+----+------+------------------+----------------------+---+--------------------+\n",
      "|         customer_id|  FN|Active|club_member_status|fashion_news_frequency|age|         postal_code|\n",
      "+--------------------+----+------+------------------+----------------------+---+--------------------+\n",
      "|00000dbacae5abe5e...|NULL|  NULL|            ACTIVE|                  NONE| 49|52043ee2162cf5aa7...|\n",
      "|0000423b00ade9141...|NULL|  NULL|            ACTIVE|                  NONE| 25|2973abc54daa8a5f8...|\n",
      "|000058a12d5b43e67...|NULL|  NULL|            ACTIVE|                  NONE| 24|64f17e6a330a85798...|\n",
      "|00005ca1c9ed5f514...|NULL|  NULL|            ACTIVE|                  NONE| 54|5d36574f52495e81f...|\n",
      "|00006413d8573cd20...| 1.0|   1.0|            ACTIVE|             Regularly| 52|25fa5ddee9aac01b3...|\n",
      "+--------------------+----+------+------------------+----------------------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "‚öôÔ∏è Running 'preprocess_customers' logic...\n",
      "‚úÖ Processing complete! Showing result sample:\n",
      "+--------------------+---+------+---+----------+------------------+----------------------+\n",
      "|         customer_id| FN|Active|age|age_bucket|club_member_status|fashion_news_frequency|\n",
      "+--------------------+---+------+---+----------+------------------+----------------------+\n",
      "|00000dbacae5abe5e...|0.0|   0.0| 49|         3|            ACTIVE|                  NONE|\n",
      "|0000423b00ade9141...|0.0|   0.0| 25|         2|            ACTIVE|                  NONE|\n",
      "|000058a12d5b43e67...|0.0|   0.0| 24|         1|            ACTIVE|                  NONE|\n",
      "|00005ca1c9ed5f514...|0.0|   0.0| 54|         3|            ACTIVE|                  NONE|\n",
      "|00006413d8573cd20...|1.0|   1.0| 52|         3|            ACTIVE|             Regularly|\n",
      "+--------------------+---+------+---+----------+------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "üîç Validation: Count of NULLs in 'age' column after processing: 0\n",
      "üéâ SUCCESS: Null imputation logic worked correctly on the full dataset.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Import the custom function you wrote in VS Code\n",
    "from src.feature_engineering import preprocess_customers\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "# We set memory to 4g to handle the large H&M dataset smoothly in Colab.\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"HM_Feature_Engineering_Colab\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Spark Session initialized.\")\n",
    "\n",
    "# 2. Define File Paths\n",
    "# The data was extracted to /content/hm_data/ in the previous download step.\n",
    "data_dir = \"/content/hm_data\"\n",
    "customers_file = os.path.join(data_dir, \"customers.csv\")\n",
    "\n",
    "# 3. Load Raw Data\n",
    "if not os.path.exists(customers_file):\n",
    "    print(f\"‚ùå Error: File not found at {customers_file}. Please run the download script first.\")\n",
    "else:\n",
    "    print(f\"üìÇ Loading data from: {customers_file}\")\n",
    "\n",
    "    # Read the CSV with header and infer schema\n",
    "    df_customers = spark.read.csv(customers_file, header=True, inferSchema=True)\n",
    "\n",
    "    # Display initial stats\n",
    "    print(f\"üìä Total Rows: {df_customers.count()}\")\n",
    "    print(\"üîπ Sample of raw data (Top 5):\")\n",
    "    df_customers.show(5)\n",
    "\n",
    "    # 4. Apply Feature Engineering Logic\n",
    "    # This calls the 'preprocess_customers' function from your repo\n",
    "    print(\"‚öôÔ∏è Running 'preprocess_customers' logic...\")\n",
    "    df_processed = preprocess_customers(df_customers)\n",
    "\n",
    "    # 5. Validate Results\n",
    "    print(\"‚úÖ Processing complete! Showing result sample:\")\n",
    "    df_processed.show(5)\n",
    "\n",
    "    # Verification: Check if Null values in 'age' are gone (Logic check)\n",
    "    null_age_count = df_processed.filter(F.col(\"age\").isNull()).count()\n",
    "    print(f\"üîç Validation: Count of NULLs in 'age' column after processing: {null_age_count}\")\n",
    "\n",
    "    if null_age_count == 0:\n",
    "        print(\"üéâ SUCCESS: Null imputation logic worked correctly on the full dataset.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è WARNING: There are still null values. Check the logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jajZM_OOMhaW",
    "outputId": "64397c62-8322-4466-f05e-bbe7e7a2b495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving customers to: /content/drive/MyDrive/ScaleStyle_Project/data/processed/customers_parquet\n"
     ]
    }
   ],
   "source": [
    "# Save Customers\n",
    "cust_output_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed/customers_parquet\"\n",
    "print(f\"üíæ Saving customers to: {cust_output_path}\")\n",
    "df_processed.write.mode(\"overwrite\").parquet(cust_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17oyrgrWsX-B",
    "outputId": "b1b2e627-3ffa-473a-c2a6-8aca2fe2a276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/ScaleStyle\n",
      "Already up to date.\n",
      "/content/ScaleStyle/data-pipeline\n"
     ]
    }
   ],
   "source": [
    "# 1. update code\n",
    "%cd /content/ScaleStyle\n",
    "!git pull\n",
    "%cd /content/ScaleStyle/data-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ex6A9SeN5uXn",
    "outputId": "74480476-0384-45a1-a284-86c86c9c50c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading articles from: /content/hm_data/articles.csv\n",
      "   Original Count: 105542\n",
      "‚öôÔ∏è Running preprocess_articles...\n",
      "‚úÖ Processing complete!\n",
      "root\n",
      " |-- article_id: integer (nullable = true)\n",
      " |-- prod_name: string (nullable = true)\n",
      " |-- product_type_name: string (nullable = true)\n",
      " |-- product_group_name: string (nullable = true)\n",
      " |-- graphical_appearance_name: string (nullable = true)\n",
      " |-- colour_group_name: string (nullable = true)\n",
      " |-- perceived_colour_value_id: integer (nullable = true)\n",
      " |-- perceived_colour_value_name: string (nullable = true)\n",
      " |-- perceived_colour_master_name: string (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- index_name: string (nullable = true)\n",
      " |-- index_group_name: string (nullable = true)\n",
      " |-- section_name: string (nullable = true)\n",
      " |-- garment_group_name: string (nullable = true)\n",
      " |-- detail_desc: string (nullable = true)\n",
      " |-- department: string (nullable = false)\n",
      " |-- index_crossed: string (nullable = false)\n",
      " |-- color: string (nullable = false)\n",
      " |-- product: string (nullable = false)\n",
      "\n",
      "+----------+-----------------+-----------------+------------------+-------------------------+-----------------+-------------------------+---------------------------+----------------------------+---------------+----------------+----------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|article_id|        prod_name|product_type_name|product_group_name|graphical_appearance_name|colour_group_name|perceived_colour_value_id|perceived_colour_value_name|perceived_colour_master_name|department_name|      index_name|index_group_name|        section_name|garment_group_name|         detail_desc|          department|       index_crossed|               color|             product|\n",
      "+----------+-----------------+-----------------+------------------+-------------------------+-----------------+-------------------------+---------------------------+----------------------------+---------------+----------------+----------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 108775015|        Strap top|         Vest top|Garment Upper body|                    Solid|            Black|                        4|                       Dark|                       Black|   Jersey Basic|      Ladieswear|      Ladieswear|Womens Everyday B...|      Jersey Basic|Jersey top with n...|Jersey Basic_Jers...|Ladieswear_Ladies...|    Black_Dark_Black|Strap top_Vest to...|\n",
      "| 108775044|        Strap top|         Vest top|Garment Upper body|                    Solid|            White|                        3|                      Light|                       White|   Jersey Basic|      Ladieswear|      Ladieswear|Womens Everyday B...|      Jersey Basic|Jersey top with n...|Jersey Basic_Jers...|Ladieswear_Ladies...|   White_Light_White|Strap top_Vest to...|\n",
      "| 108775051|    Strap top (1)|         Vest top|Garment Upper body|                   Stripe|        Off White|                        1|                Dusty Light|                       White|   Jersey Basic|      Ladieswear|      Ladieswear|Womens Everyday B...|      Jersey Basic|Jersey top with n...|Jersey Basic_Jers...|Ladieswear_Ladies...|Off White_Dusty L...|Strap top (1)_Ves...|\n",
      "| 110065001|OP T-shirt (Idro)|              Bra|         Underwear|                    Solid|            Black|                        4|                       Dark|                       Black| Clean Lingerie|Lingeries/Tights|      Ladieswear|     Womens Lingerie| Under-, Nightwear|Microfibre T-shir...|Clean Lingerie_Un...|Lingeries/Tights_...|    Black_Dark_Black|OP T-shirt (Idro)...|\n",
      "| 110065002|OP T-shirt (Idro)|              Bra|         Underwear|                    Solid|            White|                        3|                      Light|                       White| Clean Lingerie|Lingeries/Tights|      Ladieswear|     Womens Lingerie| Under-, Nightwear|Microfibre T-shir...|Clean Lingerie_Un...|Lingeries/Tights_...|   White_Light_White|OP T-shirt (Idro)...|\n",
      "+----------+-----------------+-----------------+------------------+-------------------------+-----------------+-------------------------+---------------------------+----------------------------+---------------+----------------+----------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from src.feature_engineering import preprocess_articles\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "# 1. Define Paths\n",
    "data_dir = \"/content/hm_data\"\n",
    "articles_file = os.path.join(data_dir, \"articles.csv\")\n",
    "\n",
    "# 2. Load raw data\n",
    "print(f\"üìÇ Loading articles from: {articles_file}\")\n",
    "df_articles = spark.read.csv(articles_file, header=True, inferSchema=True)\n",
    "print(f\"   Original Count: {df_articles.count()}\")\n",
    "\n",
    "# 3. execute the preprocessing logic\n",
    "print(\"‚öôÔ∏è Running preprocess_articles...\")\n",
    "df_articles_processed = preprocess_articles(df_articles)\n",
    "\n",
    "# 4. validate results\n",
    "print(\"‚úÖ Processing complete!\")\n",
    "df_articles_processed.printSchema()\n",
    "df_articles_processed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLBw2zKO5w03",
    "outputId": "28a5075b-7c34-43cb-c7e7-c175b13e0bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving to: /content/drive/MyDrive/ScaleStyle_Project/data/processed/articles_parquet\n",
      "üéâ Articles data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. define output path\n",
    "output_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed/articles_parquet\"\n",
    "\n",
    "# 2. save data\n",
    "print(f\"üíæ Saving to: {output_path}\")\n",
    "# coalesce(1) still suitable for this data size and makes viewing easier\n",
    "df_articles_processed.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(\"üéâ Articles data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpKkPNd79Goi",
    "outputId": "14adc195-30e5-4fe1-ad3c-9c32ba91f23f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/ScaleStyle\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%cd /content/ScaleStyle\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0TxCmnbLbgI",
    "outputId": "ce105106-25d8-452b-8e3f-159c6fce5144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading transactions from: /content/hm_data/transactions_train.csv\n",
      "   Original Count: 31788324\n",
      "‚öôÔ∏è Running preprocess_transactions (Aggregation & Scaling)...\n",
      "‚úÖ Processing complete! Showing sample:\n",
      "root\n",
      " |-- t_dat: date (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- article_id: integer (nullable = true)\n",
      " |-- price_scaled: double (nullable = true)\n",
      " |-- article_purchase_count_scaled: double (nullable = true)\n",
      " |-- sales_channel_id: integer (nullable = true)\n",
      "\n",
      "+----------+--------------------+----------+-------------------+-----------------------------+----------------+\n",
      "|     t_dat|         customer_id|article_id|       price_scaled|article_purchase_count_scaled|sales_channel_id|\n",
      "+----------+--------------------+----------+-------------------+-----------------------------+----------------+\n",
      "|2018-09-20|005c9fb2ba6c49b20...| 625939005| -1.275104053924997|         -0.24097982621230762|               2|\n",
      "|2018-09-20|02389a26fba684b9f...| 572533005| -0.653964362708004|         -0.24097982621230762|               2|\n",
      "|2018-09-20|023b48de81f6af9de...| 567728006|-0.4764958795031488|         -0.24097982621230762|               2|\n",
      "|2018-09-20|0247a4cfbe56ac5e1...| 493103005|-0.7426986043104316|         -0.24097982621230762|               1|\n",
      "|2018-09-20|03023b630d51f53fc...| 685687004|-0.5652301211055765|         -0.24097982621230762|               2|\n",
      "+----------+--------------------+----------+-------------------+-----------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "üíæ Saving to: /content/drive/MyDrive/ScaleStyle_Project/data/processed/transactions_parquet\n",
      "üéâ Transactions ETL Finished!\n"
     ]
    }
   ],
   "source": [
    "from src.feature_engineering import preprocess_transactions\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# 1. Initialize Spark (Allocate sufficient memory)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"HM_Transactions_ETL\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2. Define paths\n",
    "data_dir = \"/content/hm_data\"\n",
    "transactions_file = os.path.join(data_dir, \"transactions_train.csv\")\n",
    "output_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed/transactions_parquet\"\n",
    "\n",
    "# 3. Load raw data (31M+ rows)\n",
    "print(f\"üìÇ Loading transactions from: {transactions_file}\")\n",
    "# This step might take a while\n",
    "df_trans = spark.read.csv(transactions_file, header=True, inferSchema=True)\n",
    "print(f\"   Original Count: {df_trans.count()}\")\n",
    "\n",
    "# 4. Run your core logic\n",
    "print(\"‚öôÔ∏è Running preprocess_transactions (Aggregation & Scaling)...\")\n",
    "# Note: This step involves groupBy and Scaling, which is computationally expensive. Please wait patiently.\n",
    "df_trans_processed = preprocess_transactions(df_trans)\n",
    "\n",
    "# 5. Validate results\n",
    "print(\"‚úÖ Processing complete! Showing sample:\")\n",
    "df_trans_processed.printSchema()\n",
    "df_trans_processed.show(5)\n",
    "\n",
    "# 6. Save data\n",
    "print(f\"üíæ Saving to: {output_path}\")\n",
    "# Since the data volume is large, do not use coalesce(1) here; let Spark write multiple files in parallel to improve speed.\n",
    "df_trans_processed.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(\"üéâ Transactions ETL Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0D1IKXNL6l8",
    "outputId": "d91ca9ae-2902-409c-989b-a9c118d7a987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/ScaleStyle\n",
      "Already up to date.\n",
      "/content/ScaleStyle/data-pipeline\n",
      "‚è≥ Loading Parquet files...\n",
      "   Trans Count: 28813419\n",
      "‚öôÔ∏è Merging datasets...\n",
      "‚úÖ Merge Complete! Final Row Count: 28813419\n",
      "üéâ Integrity Check Passed: Row counts match.\n",
      "üíæ Saving final train data to: /content/drive/MyDrive/ScaleStyle_Project/data/processed/train_data_parquet\n",
      "üöÄ MISSION COMPLETE: Phase 1 Data Pipeline Finished!\n"
     ]
    }
   ],
   "source": [
    "%cd /content/ScaleStyle\n",
    "!git pull\n",
    "%cd /content/ScaleStyle/data-pipeline\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from src.feature_engineering import merge_datasets\n",
    "import os\n",
    "\n",
    "# 1. Init Spark (Max Memory)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"HM_Data_Merge\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    "    .config(\"spark.executor.memory\", \"12g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2. Paths\n",
    "base_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed\"\n",
    "trans_path = os.path.join(base_path, \"transactions_parquet\")\n",
    "cust_path = os.path.join(base_path, \"customers_parquet\")\n",
    "art_path = os.path.join(base_path, \"articles_parquet\")\n",
    "output_path = os.path.join(base_path, \"train_data_parquet\")\n",
    "\n",
    "# 3. Load Data\n",
    "print(\"‚è≥ Loading Parquet files...\")\n",
    "df_trans = spark.read.parquet(trans_path)\n",
    "df_cust = spark.read.parquet(cust_path)\n",
    "df_art = spark.read.parquet(art_path)\n",
    "\n",
    "print(f\"   Trans Count: {df_trans.count()}\")\n",
    "\n",
    "# 4. Merge\n",
    "print(\"‚öôÔ∏è Merging datasets...\")\n",
    "df_train = merge_datasets(df_trans, df_cust, df_art)\n",
    "\n",
    "# 5. Sanity Check\n",
    "final_count = df_train.count()\n",
    "print(f\"‚úÖ Merge Complete! Final Row Count: {final_count}\")\n",
    "\n",
    "if final_count != df_trans.count():\n",
    "    print(\"‚ö†Ô∏è WARNING: Row count changed! Check for duplicates in Cust/Art tables.\")\n",
    "else:\n",
    "    print(\"üéâ Integrity Check Passed: Row counts match.\")\n",
    "\n",
    "# 6. Save\n",
    "print(f\"üíæ Saving final train data to: {output_path}\")\n",
    "\n",
    "df_train.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(\"üöÄ MISSION COMPLETE: Data Pipeline Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7WNXNVd7RVSu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM+unbgqgQsLDjCx7YkCwTd",
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
