{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EthanGaoZhiyuan/ScaleStyle/blob/feat%2Fphase2-data-pipeline/data-pipeline/notebooks/H%26M_RecSys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vmUtYYTVk4a8"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# DATA DOWNLOAD & INITIAL ENVIRONMENT SETUP (COLAB RUNNER)\n",
        "# ------------------------------------------------------------------------------\n",
        "# Purpose: Manages the secure download and extraction of the H&M dataset.\n",
        "# Assisted by: Gemini (AI) for Colab file system and CLI integration.\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from google.colab import files, drive\n",
        "\n",
        "# --- Configuration ---\n",
        "# We define constants at the top for better maintainability (Senior Engineering practice)\n",
        "COMPETITION_NAME = \"h-and-m-personalized-fashion-recommendations\"\n",
        "DATA_DIR = Path(\"./hm_data\")\n",
        "DRIVE_MOUNT_PATH = Path(\"/content/drive\")\n",
        "DRIVE_PROJECT_PATH = DRIVE_MOUNT_PATH / \"MyDrive/ScaleStyle_Project/data\"\n",
        "\n",
        "\n",
        "def setup_kaggle_auth():\n",
        "    \"\"\"\n",
        "    Sets up Kaggle API authentication by asking the user to upload kaggle.json.\n",
        "    Moves the file to the correct location and sets permissions.\n",
        "    \"\"\"\n",
        "    kaggle_dir = Path.home() / \".kaggle\"\n",
        "    kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
        "\n",
        "    if kaggle_json_path.exists():\n",
        "        print(f\"‚úÖ Kaggle authentication found at {kaggle_json_path}\")\n",
        "        return\n",
        "\n",
        "    print(\"üîπ Please upload your 'kaggle.json' file now...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if \"kaggle.json\" not in uploaded:\n",
        "        print(\"‚ùå Error: kaggle.json was not uploaded.\")\n",
        "        return\n",
        "\n",
        "    # Create .kaggle directory and move file\n",
        "    kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
        "    os.rename(\"kaggle.json\", kaggle_json_path)\n",
        "\n",
        "    # Security: Restrict file permissions (Linux command)\n",
        "    os.chmod(kaggle_json_path, 0o600)\n",
        "    print(\"‚úÖ Kaggle authentication setup complete.\")\n",
        "\n",
        "\n",
        "def download_and_extract(use_drive_cache: bool = True):\n",
        "    \"\"\"\n",
        "    Downloads the dataset from Kaggle.\n",
        "\n",
        "    Args:\n",
        "        use_drive_cache (bool): If True, mounts Google Drive to persist the raw zip file\n",
        "                                so you don't have to download 30GB every time.\n",
        "    \"\"\"\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Option A: Use Google Drive to cache the huge zip file (Recommended)\n",
        "    if use_drive_cache:\n",
        "        print(\"üîπ Mounting Google Drive for persistent storage...\")\n",
        "        drive.mount(str(DRIVE_MOUNT_PATH))\n",
        "        DRIVE_PROJECT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        zip_path = DRIVE_PROJECT_PATH / f\"{COMPETITION_NAME}.zip\"\n",
        "\n",
        "        # Check if we already downloaded it to Drive\n",
        "        if not zip_path.exists():\n",
        "            print(f\"üîπ Downloading dataset to Google Drive ({zip_path})... this may take a while.\")\n",
        "            # Use Kaggle CLI to download to specific path\n",
        "            !kaggle competitions download -c {COMPETITION_NAME} -p {DRIVE_PROJECT_PATH}\n",
        "        else:\n",
        "            print(f\"‚úÖ Found cached dataset in Drive: {zip_path}\")\n",
        "\n",
        "    # Option B: Direct download to Colab (Ephemeral)\n",
        "    else:\n",
        "        print(\"üîπ Downloading dataset directly to Colab (Ephemeral storage)...\")\n",
        "        !kaggle competitions download -c {COMPETITION_NAME}\n",
        "        zip_path = Path(f\"{COMPETITION_NAME}.zip\")\n",
        "\n",
        "    # --- Extraction Phase ---\n",
        "    print(\"üîπ Extracting core CSV files (Skipping images for now to save space)...\")\n",
        "\n",
        "    # We use the system 'unzip' command which is faster than Python's zipfile for large files\n",
        "    # We explicitly ONLY extract the CSVs first. The images are too large (29GB+).\n",
        "    # We will handle image extraction later in the pipeline when needed.\n",
        "    zip_source = str(zip_path)\n",
        "    target_dir = str(DATA_DIR)\n",
        "\n",
        "    !unzip -q -o \"{zip_source}\" \"articles.csv\" \"customers.csv\" \"transactions_train.csv\" -d \"{target_dir}\"\n",
        "\n",
        "    print(f\"‚úÖ Extraction complete! Data is available at: {DATA_DIR}\")\n",
        "    print(f\"üìÇ Files: {os.listdir(DATA_DIR)}\")\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "# if __name__ == \"__main__\":\n",
        "#     setup_kaggle_auth()\n",
        "\n",
        "#     # Set to True to save the zip to Drive (saves bandwidth on restarts)\n",
        "#     # Set to False for a quick, one-off test\n",
        "#     download_and_extract(use_drive_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kyf-8zXrb1N",
        "outputId": "96d51695-3777-4cbd-a286-9d68920dc34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Google Drive is already mounted.\n",
            "‚úÖ Found cached dataset in Drive: /content/drive/MyDrive/ScaleStyle_Project/data/h-and-m-personalized-fashion-recommendations.zip\n",
            "‚úÖ Data already extracted at /content/hm_data. Skipping extraction.\n",
            "üìÇ Available files: ['articles.csv', 'transactions_train.csv', 'customers.csv']\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# DATA LOADER & SETUP (OPTIMIZED FOR GOOGLE DRIVE)\n",
        "# ------------------------------------------------------------------------------\n",
        "# Purpose: Mounts Google Drive and loads the H&M dataset.\n",
        "#          Skips download if data already exists in Drive to save time.\n",
        "# ==============================================================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Configuration ---\n",
        "COMPETITION_NAME = \"h-and-m-personalized-fashion-recommendations\"\n",
        "# This is where we want the data to be available for our Spark code\n",
        "LOCAL_DATA_DIR = Path(\"/content/hm_data\")\n",
        "\n",
        "# This is where the persistent data lives in your Google Drive\n",
        "# NOTE: Ensure this path matches where you saved it last time!\n",
        "DRIVE_MOUNT_POINT = \"/content/drive\"\n",
        "DRIVE_PROJECT_PATH = Path(DRIVE_MOUNT_POINT) / \"MyDrive/ScaleStyle_Project/data\"\n",
        "ZIP_FILE_PATH = DRIVE_PROJECT_PATH / f\"{COMPETITION_NAME}.zip\"\n",
        "\n",
        "\n",
        "def setup_data_from_drive():\n",
        "    \"\"\"\n",
        "    Mounts Google Drive and prepares the dataset.\n",
        "    1. Mounts Drive.\n",
        "    2. Checks if the ZIP file exists in Drive.\n",
        "    3. Extracts CSVs to the local Colab environment for fast access.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Mount Google Drive\n",
        "    if not os.path.exists(DRIVE_MOUNT_POINT):\n",
        "        print(\"üîπ Mounting Google Drive...\")\n",
        "        drive.mount(DRIVE_MOUNT_POINT)\n",
        "    else:\n",
        "        print(\"‚úÖ Google Drive is already mounted.\")\n",
        "\n",
        "    # 2. Check for the dataset in Drive\n",
        "    if not ZIP_FILE_PATH.exists():\n",
        "        print(f\"‚ùå Error: Dataset not found at {ZIP_FILE_PATH}\")\n",
        "        print(\"   Did you save it to a different folder last time?\")\n",
        "        print(\"   If this is a fresh start, you may need to run the download script once.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ Found cached dataset in Drive: {ZIP_FILE_PATH}\")\n",
        "\n",
        "    # 3. Extract to local environment (Faster IO than reading from Drive directly)\n",
        "    # We only extract if the target directory is empty or missing\n",
        "    if not LOCAL_DATA_DIR.exists():\n",
        "        print(\"üîπ Extracting data to local Colab environment (this improves speed)...\")\n",
        "        LOCAL_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Unzip command is faster than Python zipfile\n",
        "        zip_source = str(ZIP_FILE_PATH)\n",
        "        target_dir = str(LOCAL_DATA_DIR)\n",
        "\n",
        "        # Extract specific CSVs to save space/time\n",
        "        !unzip -q -o \"{zip_source}\" \"articles.csv\" \"customers.csv\" \"transactions_train.csv\" -d \"{target_dir}\"\n",
        "\n",
        "        print(f\"‚úÖ Extraction complete! Data is ready at: {LOCAL_DATA_DIR}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Data already extracted at {LOCAL_DATA_DIR}. Skipping extraction.\")\n",
        "\n",
        "    print(f\"üìÇ Available files: {os.listdir(LOCAL_DATA_DIR)}\")\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    setup_data_from_drive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0lgMz5MqBkD",
        "outputId": "34089191-e7d2-4f33-ac48-9d078a9ad236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Token retrieved successfully\n",
            "fatal: destination path 'ScaleStyle' already exists and is not an empty directory.\n",
            "\n",
            "üéâ Code downloaded to: ScaleStyle\n",
            "data-pipeline\t    docs\t     inference-service\tREADME.md\n",
            "docker-compose.yml  gateway-service  infrastructure\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# 1. Securely retrieve Token (will not be displayed on screen)\n",
        "try:\n",
        "    token = userdata.get(\"GITHUB_TOKEN\")\n",
        "    print(\"‚úÖ Token retrieved successfully\")\n",
        "except Exception:\n",
        "    print(\"‚ùå Token not found. Please check the Secrets panel settings on the left\")\n",
        "\n",
        "# 2. Configure repository information\n",
        "username = \"EthanGaoZhiyuan\"\n",
        "repo = \"ScaleStyle\"\n",
        "branch = \"feat/phase1-data-pipeline\"  # Specify the target branch\n",
        "\n",
        "# 3. Construct HTTPS URL with Token (Token used for authentication)\n",
        "# Format: https://token@github.com/username/repo.git\n",
        "clone_url = f\"https://{token}@github.com/{username}/{repo}.git\"\n",
        "\n",
        "# 4. Execute clone command\n",
        "# Use the -b flag to directly clone a specific branch\n",
        "!git clone -b main {clone_url}\n",
        "\n",
        "# 5. Verification\n",
        "if os.path.exists(repo):\n",
        "    print(f\"\\nüéâ Code downloaded to: {repo}\")\n",
        "    !ls {repo}\n",
        "else:\n",
        "    print(\"\\n‚ùå Clone failed. Please check Token permissions or repository existence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7tJSAtUqyBZ",
        "outputId": "eab18d65-bdb5-4ed6-ac40-8282b115b2c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Current Working Directory: /content/ScaleStyle/data-pipeline\n",
            "‚¨áÔ∏è Installing dependencies from requirements.txt...\n",
            "‚úÖ Added /content/ScaleStyle/data-pipeline to system path.\n",
            "‚úÖ Environment setup complete.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# 1. Change directory to the data-pipeline folder to access requirements.txt\n",
        "# This ensures pip installs the exact versions specified in your repo.\n",
        "repo_path = \"/content/ScaleStyle/data-pipeline\"\n",
        "os.chdir(repo_path)\n",
        "\n",
        "print(f\"üìÇ Current Working Directory: {os.getcwd()}\")\n",
        "\n",
        "# 2. Install dependencies\n",
        "# Using -q to keep the output clean.\n",
        "print(\"‚¨áÔ∏è Installing dependencies from requirements.txt...\")\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# 3. Add the project root to system path\n",
        "# This allows Python to recognize 'src' as a module so we can do:\n",
        "# \"from src.feature_engineering import ...\"\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)\n",
        "    print(f\"‚úÖ Added {repo_path} to system path.\")\n",
        "\n",
        "print(\"‚úÖ Environment setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgk8WzZArCsn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Import the custom function you wrote in VS Code\n",
        "from src.feature_engineering import preprocess_customers\n",
        "\n",
        "# 1. Initialize Spark Session\n",
        "# We set memory to 4g to handle the large H&M dataset smoothly in Colab.\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"HM_Feature_Engineering_Colab\")\n",
        "    .config(\"spark.driver.memory\", \"4g\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Spark Session initialized.\")\n",
        "\n",
        "# 2. Define File Paths\n",
        "# The data was extracted to /content/hm_data/ in the previous download step.\n",
        "data_dir = \"/content/hm_data\"\n",
        "customers_file = os.path.join(data_dir, \"customers.csv\")\n",
        "\n",
        "# 3. Load Raw Data\n",
        "if not os.path.exists(customers_file):\n",
        "    print(f\"‚ùå Error: File not found at {customers_file}. Please run the download script first.\")\n",
        "else:\n",
        "    print(f\"üìÇ Loading data from: {customers_file}\")\n",
        "\n",
        "    # Read the CSV with header and infer schema\n",
        "    df_customers = spark.read.csv(customers_file, header=True, inferSchema=True)\n",
        "\n",
        "    # Display initial stats\n",
        "    print(f\"üìä Total Rows: {df_customers.count()}\")\n",
        "    print(\"üîπ Sample of raw data (Top 5):\")\n",
        "    df_customers.show(5)\n",
        "\n",
        "    # 4. Apply Feature Engineering Logic\n",
        "    # This calls the 'preprocess_customers' function from your repo\n",
        "    print(\"‚öôÔ∏è Running 'preprocess_customers' logic...\")\n",
        "    df_processed = preprocess_customers(df_customers)\n",
        "\n",
        "    # 5. Validate Results\n",
        "    print(\"‚úÖ Processing complete! Showing result sample:\")\n",
        "    df_processed.show(5)\n",
        "\n",
        "    # Verification: Check if Null values in 'age' are gone (Logic check)\n",
        "    null_age_count = df_processed.filter(F.col(\"age\").isNull()).count()\n",
        "    print(f\"üîç Validation: Count of NULLs in 'age' column after processing: {null_age_count}\")\n",
        "\n",
        "    if null_age_count == 0:\n",
        "        print(\"üéâ SUCCESS: Null imputation logic worked correctly on the full dataset.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: There are still null values. Check the logic.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jajZM_OOMhaW"
      },
      "outputs": [],
      "source": [
        "# Save Customers\n",
        "cust_output_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed/customers_parquet\"\n",
        "print(f\"üíæ Saving customers to: {cust_output_path}\")\n",
        "df_processed.write.mode(\"overwrite\").parquet(cust_output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17oyrgrWsX-B"
      },
      "outputs": [],
      "source": [
        "# 1. update code\n",
        "%cd /content/ScaleStyle\n",
        "!git pull\n",
        "%cd /content/ScaleStyle/data-pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex6A9SeN5uXn"
      },
      "outputs": [],
      "source": [
        "from src.feature_engineering import preprocess_articles\n",
        "from pyspark.sql import functions as F\n",
        "import os\n",
        "\n",
        "# 1. Define Paths\n",
        "data_dir = \"/content/hm_data\"\n",
        "articles_file = os.path.join(data_dir, \"articles.csv\")\n",
        "\n",
        "# 2. Load raw data\n",
        "print(f\"üìÇ Loading articles from: {articles_file}\")\n",
        "df_articles = spark.read.csv(articles_file, header=True, inferSchema=True)\n",
        "print(f\"   Original Count: {df_articles.count()}\")\n",
        "\n",
        "# 3. execute the preprocessing logic\n",
        "print(\"‚öôÔ∏è Running preprocess_articles...\")\n",
        "df_articles_processed = preprocess_articles(df_articles)\n",
        "\n",
        "# 4. validate results\n",
        "print(\"‚úÖ Processing complete!\")\n",
        "df_articles_processed.printSchema()\n",
        "df_articles_processed.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLBw2zKO5w03"
      },
      "outputs": [],
      "source": [
        "# 1. define output path\n",
        "output_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed/articles_parquet\"\n",
        "\n",
        "# 2. save data\n",
        "print(f\"üíæ Saving to: {output_path}\")\n",
        "# coalesce(1) still suitable for this data size and makes viewing easier\n",
        "df_articles_processed.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(\"üéâ Articles data saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpKkPNd79Goi"
      },
      "outputs": [],
      "source": [
        "%cd /content/ScaleStyle\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0TxCmnbLbgI",
        "outputId": "3b668b6e-bdc4-47e5-d3ea-a148bd46a237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loading transactions from: /content/hm_data/transactions_train.csv\n",
            "   Original Count: 31788324\n",
            "‚öôÔ∏è Running preprocess_transactions (Aggregation & Scaling)...\n",
            "‚úÖ Processing complete! Showing sample:\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- article_id: integer (nullable = true)\n",
            " |-- total_decay_weight: double (nullable = true)\n",
            " |-- article_purchase_count: long (nullable = false)\n",
            " |-- last_purchase_date: date (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- sales_channel_id: integer (nullable = true)\n",
            "\n",
            "+--------------------+----------+-------------------+----------------------+------------------+--------------------+----------------+\n",
            "|         customer_id|article_id| total_decay_weight|article_purchase_count|last_purchase_date|               price|sales_channel_id|\n",
            "+--------------------+----------+-------------------+----------------------+------------------+--------------------+----------------+\n",
            "|016d8f0519d9e0572...| 527687006|0.42895454691819734|                     2|        2018-09-21| 0.10167796610169492|               1|\n",
            "|02389a26fba684b9f...| 569997016|0.21436227224008575|                     1|        2018-09-20|0.013542372881355931|               2|\n",
            "|0389e2e5ce41e5886...| 673776004|0.21436227224008575|                     1|        2018-09-20|0.050830508474576264|               1|\n",
            "|03ee4315d16cd08e5...| 699566001| 0.4291850434538344|                     2|        2018-09-22| 0.05930508474576271|               1|\n",
            "|04901bc197bed482a...| 484398001|0.21436227224008575|                     1|        2018-09-20|0.033881355932203386|               1|\n",
            "+--------------------+----------+-------------------+----------------------+------------------+--------------------+----------------+\n",
            "only showing top 5 rows\n",
            "üíæ Saving to: /content/drive/MyDrive/ScaleStyle_Project/data/processed/transactions_parquet\n",
            "üéâ Transactions ETL Finished!\n"
          ]
        }
      ],
      "source": [
        "from src.feature_engineering import preprocess_transactions\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# 1. Initialize Spark (Allocate sufficient memory)\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"HM_Transactions_ETL\")\n",
        "    .config(\"spark.driver.memory\", \"8g\")\n",
        "    .config(\"spark.executor.memory\", \"8g\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# 2. Define paths\n",
        "data_dir = \"/content/hm_data\"\n",
        "transactions_file = os.path.join(data_dir, \"transactions_train.csv\")\n",
        "output_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed/transactions_parquet\"\n",
        "\n",
        "# 3. Load raw data (31M+ rows)\n",
        "print(f\"üìÇ Loading transactions from: {transactions_file}\")\n",
        "# This step might take a while\n",
        "df_trans = spark.read.csv(transactions_file, header=True, inferSchema=True)\n",
        "print(f\"   Original Count: {df_trans.count()}\")\n",
        "\n",
        "# 4. Run your core logic\n",
        "print(\"‚öôÔ∏è Running preprocess_transactions (Aggregation & Scaling)...\")\n",
        "# Note: This step involves groupBy and Scaling, which is computationally expensive. Please wait patiently.\n",
        "df_trans_processed = preprocess_transactions(df_trans)\n",
        "\n",
        "# 5. Validate results\n",
        "print(\"‚úÖ Processing complete! Showing sample:\")\n",
        "df_trans_processed.printSchema()\n",
        "df_trans_processed.show(5)\n",
        "\n",
        "# 6. Save data\n",
        "print(f\"üíæ Saving to: {output_path}\")\n",
        "# Since the data volume is large, do not use coalesce(1) here; let Spark write multiple files in parallel to improve speed.\n",
        "df_trans_processed.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(\"üéâ Transactions ETL Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0D1IKXNL6l8",
        "outputId": "e656a50d-3f7d-4bfc-ae6a-631af7f6c5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ScaleStyle\n",
            "Already up to date.\n",
            "/content/ScaleStyle/data-pipeline\n",
            "‚è≥ Loading Parquet files...\n",
            "   Trans Count: 27306439\n",
            "‚öôÔ∏è Merging datasets...\n",
            "‚úÖ Merge Complete! Final Row Count: 27306439\n",
            "üéâ Integrity Check Passed: Row counts match.\n",
            "üíæ Saving final train data to: /content/drive/MyDrive/ScaleStyle_Project/data/processed/train_data_parquet\n",
            "üöÄ MISSION COMPLETE: Data Pipeline Finished!\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ScaleStyle\n",
        "!git pull\n",
        "%cd /content/ScaleStyle/data-pipeline\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from src.feature_engineering import merge_datasets\n",
        "import os\n",
        "\n",
        "# 1. Init Spark (Max Memory)\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"HM_Data_Merge\")\n",
        "    .config(\"spark.driver.memory\", \"12g\")\n",
        "    .config(\"spark.executor.memory\", \"12g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# 2. Paths\n",
        "base_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed\"\n",
        "trans_path = os.path.join(base_path, \"transactions_parquet\")\n",
        "cust_path = os.path.join(base_path, \"customers_parquet\")\n",
        "art_path = os.path.join(base_path, \"articles_parquet\")\n",
        "output_path = os.path.join(base_path, \"train_data_parquet\")\n",
        "\n",
        "# 3. Load Data\n",
        "print(\"‚è≥ Loading Parquet files...\")\n",
        "df_trans = spark.read.parquet(trans_path)\n",
        "df_cust = spark.read.parquet(cust_path)\n",
        "df_art = spark.read.parquet(art_path)\n",
        "\n",
        "print(f\"   Trans Count: {df_trans.count()}\")\n",
        "\n",
        "# 4. Merge\n",
        "print(\"‚öôÔ∏è Merging datasets...\")\n",
        "df_train = merge_datasets(df_trans, df_cust, df_art)\n",
        "\n",
        "# 5. Sanity Check\n",
        "final_count = df_train.count()\n",
        "print(f\"‚úÖ Merge Complete! Final Row Count: {final_count}\")\n",
        "\n",
        "if final_count != df_trans.count():\n",
        "    print(\"‚ö†Ô∏è WARNING: Row count changed! Check for duplicates in Cust/Art tables.\")\n",
        "else:\n",
        "    print(\"üéâ Integrity Check Passed: Row counts match.\")\n",
        "\n",
        "# 6. Save\n",
        "print(f\"üíæ Saving final train data to: {output_path}\")\n",
        "\n",
        "df_train.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(\"üöÄ MISSION COMPLETE: Data Pipeline Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ScaleStyle\n",
        "!git pull\n",
        "%cd /content/ScaleStyle/data-pipeline\n",
        "\n",
        "import importlib\n",
        "import src.feature_engineering\n",
        "\n",
        "importlib.reload(src.feature_engineering)\n",
        "from src.feature_engineering import preprocess_transactions, merge_datasets"
      ],
      "metadata": {
        "id": "MocTwVrmxk98",
        "outputId": "0c0571a1-e720-42ce-c44c-cb381dd0fa1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ScaleStyle\n",
            "Already up to date.\n",
            "/content/ScaleStyle/data-pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7WNXNVd7RVSu",
        "outputId": "cdebb43c-93b9-4077-856e-08c9c1695c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Loading Phase 1 positive samples and feature tables...\n",
            "‚öôÔ∏è Generating negative samples (Negative Sampling)...\n",
            "‚öôÔ∏è Backfilling negative sample features...\n",
            "üîß Detected missing columns in negative samples: ['last_purchase_date', 'article_purchase_count']\n",
            "   Auto-filling missing columns (0 for numbers, 1970-01-01 for dates)...\n",
            "üìä Stats: Positives 27306439 rows, Negatives 135467852 rows\n",
            "üíæ Saving final training set to: /content/drive/MyDrive/ScaleStyle_Project/data/processed/train_data_with_negatives_parquet\n",
            "üöÄ Phase 2 Complete! Data saved successfully.\n"
          ]
        }
      ],
      "source": [
        "from src.feature_engineering import generate_negative_samples\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DateType, TimestampType\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/ScaleStyle_Project/data/processed\"\n",
        "train_data_path = f\"{base_path}/train_data_parquet\"\n",
        "cust_path = f\"{base_path}/customers_parquet\"\n",
        "art_path = f\"{base_path}/articles_parquet\"\n",
        "final_output_path = f\"{base_path}/train_data_with_negatives_parquet\"\n",
        "\n",
        "# 2. Load data\n",
        "print(\"‚è≥ Loading Phase 1 positive samples and feature tables...\")\n",
        "df_positives = spark.read.parquet(train_data_path)\n",
        "df_cust = spark.read.parquet(cust_path)\n",
        "df_art = spark.read.parquet(art_path)\n",
        "\n",
        "# Ensure positive samples have label=1\n",
        "if \"label\" not in df_positives.columns:\n",
        "    df_positives = df_positives.withColumn(\"label\", F.lit(1))\n",
        "\n",
        "# 3. Generate Negative Samples\n",
        "# ratio=4 means 1 positive : 4 negatives\n",
        "print(\"‚öôÔ∏è Generating negative samples (Negative Sampling)...\")\n",
        "df_neg_ids = generate_negative_samples(df_positives, ratio=4)\n",
        "\n",
        "# 4. Feature Backfill\n",
        "print(\"‚öôÔ∏è Backfilling negative sample features...\")\n",
        "# Join customer and article features back to the negative IDs\n",
        "df_neg_full = df_neg_ids.join(df_cust, on=\"customer_id\", how=\"left\") \\\n",
        "                        .join(df_art, on=\"article_id\", how=\"left\")\n",
        "\n",
        "# Assign label=0 for negative samples\n",
        "df_neg_full = df_neg_full.withColumn(\"label\", F.lit(0))\n",
        "\n",
        "# Identify columns present in Positives but missing in Negatives (e.g., price, date)\n",
        "pos_cols = set(df_positives.columns)\n",
        "neg_cols = set(df_neg_full.columns)\n",
        "missing_cols = list(pos_cols - neg_cols)\n",
        "\n",
        "print(f\"üîß Detected missing columns in negative samples: {missing_cols}\")\n",
        "print(\"   Auto-filling missing columns (0 for numbers, 1970-01-01 for dates)...\")\n",
        "\n",
        "for col_name in missing_cols:\n",
        "    # Get the data type from the positive dataset\n",
        "    target_type = df_positives.schema[col_name].dataType\n",
        "\n",
        "    # Check type to decide how to fill\n",
        "    if isinstance(target_type, (DateType, TimestampType)):\n",
        "        # For Dates: Use a dummy default date (Epoch)\n",
        "        default_val = F.lit(\"1970-01-01\").cast(target_type)\n",
        "    else:\n",
        "        # For Numbers (Int, Double, Long): Use 0\n",
        "        default_val = F.lit(0).cast(target_type)\n",
        "\n",
        "    df_neg_full = df_neg_full.withColumn(col_name, default_val)\n",
        "\n",
        "# 5. Merge Data (Union)\n",
        "# Ensure column order is identical for Union\n",
        "common_columns = df_positives.columns\n",
        "df_neg_final = df_neg_full.select(*common_columns)\n",
        "\n",
        "print(f\"üìä Stats: Positives {df_positives.count()} rows, Negatives {df_neg_final.count()} rows\")\n",
        "\n",
        "# Union\n",
        "df_final_train = df_positives.unionByName(df_neg_final)\n",
        "\n",
        "# 6. Save Final Result\n",
        "print(f\"üíæ Saving final training set to: {final_output_path}\")\n",
        "# Repartition to prevent OOM\n",
        "df_final_train.repartition(100).write.mode(\"overwrite\").parquet(final_output_path)\n",
        "\n",
        "print(\"üöÄ Phase 2 Complete! Data saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ooJhhnLf7QtL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}