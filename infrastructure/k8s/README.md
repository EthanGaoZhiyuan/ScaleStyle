# Week 4: Kubernetes Deployment Guide

**Timeline**: February 1-3, 2026 (Sydney Time)  
**Goal**: Production-grade K8s deployment with autoscaling

---

## üìã Quick Start

```bash
# 1. Setup prerequisites (metrics-server, ingress)
make k8s-setup

# 2. Deploy all services
make k8s-apply

# 3. Install Milvus
make milvus-install

# 4. Initialize data
make data-init

# 5. Deploy HPA
make hpa-deploy

# 6. Check status
make k8s-status

# 7. Test API
make test-api
```

**One-command deployment** (does all above):
```bash
make deploy-all
```

---

## üóÇÔ∏è Directory Structure

```
infrastructure/k8s/
‚îú‚îÄ‚îÄ 00-namespace.yaml              # Namespace definition
‚îú‚îÄ‚îÄ 10-configmap.yaml              # Environment variables (single source of truth)
‚îú‚îÄ‚îÄ 20-redis.yaml                  # Redis deployment + service
‚îú‚îÄ‚îÄ 30-inference.yaml              # Ray Serve deployment + service
‚îú‚îÄ‚îÄ 40-gateway.yaml                # Spring Boot gateway + service
‚îú‚îÄ‚îÄ 50-ingress.yaml                # Nginx Ingress (expose gateway)
‚îú‚îÄ‚îÄ 60-init-job.yaml               # Data initialization job
‚îú‚îÄ‚îÄ 70-gateway-hpa.yaml            # Horizontal Pod Autoscaler
‚îú‚îÄ‚îÄ 80-raycluster.yaml             # KubeRay (optional advanced)
‚îî‚îÄ‚îÄ helm-values/
    ‚îú‚îÄ‚îÄ milvus-standalone.yaml     # Milvus Helm values
    ‚îî‚îÄ‚îÄ kuberay-operator.yaml      # KubeRay operator values
```

---

## üìÖ Day 1 (Feb 1): K8s YAML Manifests

### Task 1: Namespace + Label Standards ‚úÖ

All resources use consistent labels:
- `app: scalestyle`
- `component: gateway|inference|redis|milvus`
- `version: <git-sha>`

### Task 2: ConfigMap ‚úÖ

**Single source of truth** for all environment variables:
- Redis: connection settings
- Milvus: connection + collection settings
- Generation: LLM configuration
- Observability: OTel, Prometheus endpoints

**Key fix preserved**: `SPRING_HTTP_CLIENT_FACTORY=simple` for Ray Serve compatibility

### Task 3-6: Core Services ‚úÖ

| Service | Replicas | Ports | Health Checks |
|---------|----------|-------|---------------|
| Redis | 1 | 6379 | redis-cli ping |
| Inference | 1 | 8000, 8265 | /readyz, /healthz |
| Gateway | 2 | 8080 | /actuator/health/readiness |
| Ingress | - | 80 | - |

**Resource Limits**:
- Gateway: 200m-1000m CPU, 512Mi-1Gi RAM
- Inference: 500m-2000m CPU, 1Gi-4Gi RAM
- Redis: 100m-500m CPU, 128Mi-512Mi RAM

---

## üìÖ Day 2 (Feb 2): Deployment Automation

### Makefile Commands

```bash
# Setup
make k8s-setup           # Install prerequisites
make k8s-apply           # Deploy core services
make k8s-delete          # Delete services
make k8s-status          # Show status
make clean-all           # Complete cleanup

# Milvus
make milvus-install      # Install via Helm
make milvus-uninstall    # Uninstall

# Data
make data-init           # Load initial data

# Logs & Debug
make k8s-logs-gateway    # Tail gateway logs
make k8s-logs-inference  # Tail inference logs
make k8s-logs-redis      # Tail Redis logs
make k8s-describe-gateway    # Describe gateway deployment
make k8s-describe-inference  # Describe inference deployment

# Docker Images
make build-images DOCKERHUB_USER=<your-username>
make push-images DOCKERHUB_USER=<your-username>

# Testing
make test-api            # Test recommendation API
make port-forward-gateway    # Forward gateway to localhost:8080
make port-forward-inference  # Forward inference to localhost:8000
```

### Milvus Deployment Strategy

**Recommended**: Use Helm chart (production-ready)

```bash
helm repo add milvus https://zilliztech.github.io/milvus-helm/
helm repo update
make milvus-install
```

**Configuration** ([milvus-standalone.yaml](helm-values/milvus-standalone.yaml)):
- Standalone mode (1 pod)
- etcd for metadata
- MinIO for object storage
- 20Gi persistence
- Resource limits: 2 CPU, 4Gi RAM

### Data Initialization

**Job**: [60-init-job.yaml](60-init-job.yaml)

**What it does**:
1. Wait for Redis + Milvus to be ready
2. Load popular items into Redis (ZSET format)
3. Create/verify Milvus collection
4. Load embeddings (production: from S3/object storage)

**Run**: `make data-init`

**Verify**:
```bash
kubectl logs -n scalestyle job/data-init
```

---

## üìÖ Day 3 (Feb 3): Autoscaling

### Gateway HPA ‚úÖ

**File**: [70-gateway-hpa.yaml](70-gateway-hpa.yaml)

**Configuration**:
- Min replicas: 2
- Max replicas: 8
- CPU target: 60%
- Memory target: 75%

**Scaling behavior**:
- Scale up: Add 50% or 2 pods (max), wait 30s
- Scale down: Remove 50% or 1 pod (min), wait 5min

**Deploy**: `make hpa-deploy`

**Watch**: `kubectl get hpa -n scalestyle -w`

### Trigger Scaling (Demo)

**Method 1: CPU Load**
```bash
# Generate load with ab (Apache Bench)
ab -n 10000 -c 50 http://<ingress-ip>/api/recommendation/search?query=dress&k=5
```

**Method 2: Locust (recommended)**
```python
# locustfile.py
from locust import HttpUser, task, between

class RecommendationUser(HttpUser):
    wait_time = between(0.1, 0.5)
    
    @task
    def search(self):
        self.client.get("/api/recommendation/search?query=dress&k=5")
```

```bash
locust -f locustfile.py --host=http://<ingress-ip> --users=100 --spawn-rate=10
```

**Expected result**:
- Gateway pods: 2 ‚Üí 4 ‚Üí 6 ‚Üí 8 (as CPU increases)
- After load stops: 8 ‚Üí 6 ‚Üí 4 ‚Üí 2 (gradual scale down)

### KubeRay Autoscaler (Optional Advanced)

**Files**:
- [80-raycluster.yaml](80-raycluster.yaml)
- [helm-values/kuberay-operator.yaml](helm-values/kuberay-operator.yaml)

**Installation**:
```bash
# Install operator
helm repo add kuberay https://ray-project.github.io/kuberay-helm/
helm install kuberay-operator kuberay/kuberay-operator \
  -f infrastructure/k8s/helm-values/kuberay-operator.yaml

# Deploy RayCluster
kubectl apply -f infrastructure/k8s/80-raycluster.yaml
```

**Benefits**:
- Ray-native autoscaling (better than K8s HPA for Ray)
- Automatic worker scaling (1-5 workers)
- Ray dashboard integration

**When to use**:
- If you want to showcase advanced Ray knowledge
- Production deployments with high inference load
- Need fine-grained control over Ray workers

**When to skip** (Week 4):
- Time-constrained (HPA is enough for demo)
- Complexity not needed for portfolio
- Can be added in Week 5

---

## üöÄ Deployment Workflows

### Minikube Deployment

```bash
# 1. Start Minikube with sufficient resources
minikube start --cpus=4 --memory=8192 --disk-size=40g

# 2. Enable addons
minikube addons enable metrics-server
minikube addons enable ingress

# 3. Build images (option A: use Minikube's Docker)
eval $(minikube docker-env)
make build-images

# OR (option B: push to Docker Hub and pull)
make push-images DOCKERHUB_USER=<your-username>

# 4. Update image names in YAML files
# Edit 30-inference.yaml and 40-gateway.yaml
# Change: your-dockerhub-username ‚Üí <actual-username>

# 5. Deploy everything
make deploy-all

# 6. Get Ingress IP
minikube ip
# Add to /etc/hosts: <ip> scalestyle.local

# 7. Test
curl http://scalestyle.local/api/recommendation/search?query=dress&k=5
```

### EKS Deployment (AWS)

```bash
# 1. Create EKS cluster (eksctl or Terraform)
eksctl create cluster \
  --name scalestyle \
  --region us-west-2 \
  --nodegroup-name standard-workers \
  --node-type t3.large \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 5

# 2. Install metrics-server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# 3. Install AWS Load Balancer Controller
# Follow: https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html

# 4. Update ingress.yaml for ALB
# Uncomment ALB annotations
# Change ingressClassName: nginx ‚Üí alb

# 5. Deploy
make deploy-all

# 6. Get ALB DNS
kubectl get ingress -n scalestyle scalestyle-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'

# 7. Test
curl http://<alb-dns>/api/recommendation/search?query=dress&k=5
```

---

## üîç Troubleshooting

### Pods Not Starting

```bash
# Check pod status
kubectl get pods -n scalestyle

# Describe pod
kubectl describe pod <pod-name> -n scalestyle

# Check logs
kubectl logs <pod-name> -n scalestyle

# Common issues:
# 1. Image pull failed ‚Üí Check image name/tag
# 2. CrashLoopBackOff ‚Üí Check logs for application errors
# 3. Pending ‚Üí Check resources/node capacity
```

### HPA Not Working

```bash
# Check HPA status
kubectl get hpa -n scalestyle

# Common issues:
# 1. metrics-server not installed
minikube addons enable metrics-server

# 2. Resource requests not set
# ‚Üí Ensure deployment has resources.requests

# 3. HPA shows <unknown>
kubectl top pods -n scalestyle  # Should show CPU/memory
```

### Milvus Connection Failed

```bash
# Check Milvus status
kubectl get pods -n scalestyle -l app.kubernetes.io/name=milvus

# Check Milvus logs
kubectl logs -n scalestyle -l app.kubernetes.io/name=milvus

# Port-forward to test
kubectl port-forward -n scalestyle svc/milvus 19530:19530
# Then test from localhost
```

### Data Init Job Failed

```bash
# Check job status
kubectl get job -n scalestyle

# Check job logs
kubectl logs -n scalestyle job/data-init

# Re-run job (delete and reapply)
kubectl delete job data-init -n scalestyle
make data-init
```

---

## üìä Demo Checklist

### Before Demo

- [ ] All pods Running (kubectl get pods -n scalestyle)
- [ ] HPA deployed (kubectl get hpa -n scalestyle)
- [ ] Data loaded (check Redis & Milvus)
- [ ] API responding (curl test)
- [ ] Metrics-server working (kubectl top nodes)

### During Demo

1. **Show initial state**:
   ```bash
   kubectl get pods -n scalestyle
   # Show 2 gateway pods
   ```

2. **Start load test**:
   ```bash
   locust -f locustfile.py --host=http://<ingress-ip>
   # Or use ab/hey
   ```

3. **Watch HPA**:
   ```bash
   kubectl get hpa -n scalestyle -w
   # Show metrics increasing
   ```

4. **Show pods scaling**:
   ```bash
   kubectl get pods -n scalestyle -l component=gateway -w
   # Show 2 ‚Üí 4 ‚Üí 6 pods
   ```

5. **Show API still working**:
   ```bash
   curl http://<ingress-ip>/api/recommendation/search?query=dress&k=5 | jq
   ```

6. **Stop load, watch scale down**:
   ```bash
   # Stop locust
   kubectl get hpa -n scalestyle -w
   # Show gradual scale down after 5min
   ```

### Screenshots for Portfolio

- ‚úÖ kubectl get all -n scalestyle (full system view)
- ‚úÖ kubectl get hpa -n scalestyle (autoscaling)
- ‚úÖ kubectl top pods -n scalestyle (resource usage)
- ‚úÖ Locust dashboard (load test results)
- ‚úÖ API response (successful recommendation)

---

## üéØ Success Criteria

**Day 1 (Feb 1)**:
- [x] All YAML manifests created
- [x] kubectl apply -f k8s/ works
- [x] All pods reach Running/Ready

**Day 2 (Feb 2)**:
- [x] Makefile with one-command deployment
- [x] Milvus Helm installation
- [x] Data init job completes successfully
- [x] API returns results (even if fallback to popular)

**Day 3 (Feb 3)**:
- [x] HPA deployed and functional
- [x] Load test triggers scaling (2‚Üí8 pods)
- [x] Metrics visible (CPU, memory, replicas)
- [x] Screenshots for portfolio

---

## üìö Next Steps (Week 5)

After Week 4, consider:
- **Monitoring**: Prometheus + Grafana in K8s
- **Tracing**: Jaeger distributed tracing
- **Logging**: EFK stack (Elasticsearch, Fluentd, Kibana)
- **CI/CD**: GitHub Actions ‚Üí K8s deployment
- **Security**: Network policies, RBAC, secrets management
- **Multi-region**: Traffic splitting, geo-routing

---

**Created**: 2026-02-02  
**Status**: ‚úÖ Complete K8s deployment ready  
**Next**: Deploy to Minikube/EKS and demo autoscaling
